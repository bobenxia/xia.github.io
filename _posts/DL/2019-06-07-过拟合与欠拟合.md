---
layout: post
title: 过拟合与欠拟合
category: DL
tags: DL
keywords: overfit underfit
description:
---

## 过拟合与欠拟合

- 欠拟合指模型不能在训练集上获得足够低的训练误差；

- 过拟合指模型的训练误差与测试误差（泛化误差）之间差距过大；

- - 反映在评价指标上，就是模型在训练集上表现良好，但是在测试集和新数据上表现一般（泛化能力差）；

## 降低过拟合风险的方法

所有为了减少测试误差的策略统称为正则化方法，这些方法可能会以增大训练误差为代价。

- 数据增强
- - 图像：平移、旋转、缩放
- - 利用生成对抗网络（GAN）生成新数据
- - NLP：利用机器翻译生成新数据

- 降低模型复杂度
- - 神经网络：减少网络层、神经元个数
- - 决策树：降低树的深度、剪枝

- 权值约束（添加正则化项）
- - L1 正则化
- - L2 正则化

-  集成学习
- - 神经网络：Dropout
- - 决策树：随机森林、GBDT

- 提前终止

## 降低欠拟合风险的方法

- 加入新的特征
- - 交叉特征、多项式特征、...
- - 深度学习：因子分解机、Deep-Crossing、自编码器

- 增加模型复杂度
- - 线性模型：添加高次项
- - 神经网络：增加网络层数、神经元个数

- 减小正则化项的系数
- - 添加正则化项是为了限制模型的学习能力，减小正则化项的系数则可以放宽这个限制
- - 模型通常更倾向于更大的权重，更大的权重可以使模型更好的拟合数据