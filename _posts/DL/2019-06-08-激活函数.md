---
layout: post
title: 激活函数
category: DL
tags: DL
keywords: activation function
description:
---

## 激活函数的作用

向网络中加入非线性因素，加强网络的表示能力，解决线性模型无法解决的问题

#### 为什么加入非线性因素能够加强网络的表示能力？——神经网络的万能近似定理

- 神经网络的万能近似定理：主要神经网络具有至少一个非线性隐藏层，那么只要给予网络足够数量的隐藏单元，它就可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的函数。

- 如果不使用非线性激活函数，那么每一层输出都是上层输入的线性组合；此时无论网络有多少层，其整体也将是线性的，这会导致失去万能近似的性质

- 但仅部分层是纯线性是可以接受的，这有助于减少网络中的参数。

## 常见的激活函数

### Sigmoid（Logistic）

### ReLU

### ReLU 相比 sigmoid 的优势

- 避免梯度消失
- - sigmoid函数在输入取绝对值非常大的正值或负值时会出现饱和现象——在图像上表现为变得很平，此时函数会对输入的微小变化不敏感——从而造成梯度消失；
- - ReLU 的导数始终是一个常数——负半区为 0，正半区为 1——所以不会发生梯度消失现象
- 减缓过拟合
- - ReLU 在负半区的输出为 0。一旦神经元的激活值进入负半区，那么该激活值就不会产生梯度/不会被训练，造成了网络的稀疏性——稀疏激活
- - 这有助于减少参数的相互依赖，缓解过拟合问题的发生
- 加速计算
- - ReLU 的求导不涉及浮点运算，所以速度更快