---
layout: post
title: 统计学习方法-第1章-绪论
category: ML
tags: ML
keywords: 监督学习
description:
---

## 统计学习分类

|分类标准|类型|
|:---:|:---:|
|基本分类|监督学习、无监督学习、强化学习|
|按模型分类|概率模型、非概率模型<br>（在监督学习中，概率模型是生成模型，非概率模型是判别模型）|
|按算法分类|在线学习、批量学习|
|按技巧分类|贝叶斯学习、核方法|

## 统计学习方法三要素

### 模型

在监督学习过程中，模型就是所要学习的**条件概率分布**或者**决策函数**。

||假设空间 $\mathcal { F }$ | 输入空间 $\mathcal { X }$ | 输出空间 $\mathcal { Y }$ |参数空间|
|----|-----|----|----|----|
|决策函数|$$\mathcal { F } = \left\{ f _ { \theta } \mid Y = f _ { \theta } ( x ) , \theta \in \mathbf { R } ^ { n } \right\}$$ |变量|变量| $$\mathbf { R } ^ { n }$$ |
|条件概率分布| $$\mathcal { F } = \left\{ P \mid P _ { \theta } ( Y \mid X ) , \theta \in \mathbf { R } ^ { n } \right\}$$ |随机变量|随机变量| $$\mathbf { R } ^ { n }$$ |

### 策略

#### 损失函数与风险函数

**损失函数**度量模型**一次预测**的好坏，**风险函数**度量**平均意义**下模型预测的好坏。

1. 损失函数(loss function)或代价函数(cost function)

    损失函数定义为给定输入$X$的**预测值$f(X)$**和**真实值$Y$**之间的**非负实值**函数，记作$L(Y,f(X))$

2. 风险函数(risk function)或期望损失(expected loss)

    这个和模型的泛化误差的形式是一样的

    $$
    R _ { \exp } ( f ) = E _ { p } [ L ( Y , f ( X ) ) ] = \int _ { \mathcal { X } \times \mathcal { Y } } L ( y , f ( x ) ) P ( x , y ) \mathrm { d } x \mathrm { d } y
    $$

    模型 $f(X)$ 关于联合分布 $P(X,Y)$ 的**平均意义下的**损失(**期望**损失)，但是因为$P(X,Y)$是未知的，所以前面的用词是**期望**，以及**平均意义下的**。

    这个表示其实就是损失的均值，反映了对整个数据的预测效果的好坏，$P(x,y)$转换成$\frac {\nu(X=x, Y=y)} {N}$。

3. **经验风险**(empirical risk)或**经验损失**(empirical loss)

    $$
    R _ { e m p } ( f ) = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } L \left( y _ { i } , f \left( x _ { i } \right) \right)
    $$

    模型 $f$ 关于**训练样本集**的平均损失。根据大数定律，当样本容量 N 趋于无穷大时，经验风险趋于期望风险

4. **结构风险**(structural risk)

    $$
    R _ { s r m } ( f ) = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } L \left( y _ { i } , f \left( x _ { i } \right) \right) + \lambda J ( f )
    $$

    $J(f)$ 为模型复杂度, $\lambda \geqslant 0$是系数，用以权衡经验风险和模型复杂度。

#### 常用损失函数

- 0-1损失

<div>
$$
L ( Y , f ( X ) ) = \left\{ \begin{array} { l } { 1 , Y \neq f ( X ) } \\ { 0 , Y = f ( X ) } \end{array} \right.
$$
</div>

- 平方损失

$$
L ( Y , f ( X ) ) = ( Y - f ( X ) ) ^ { 2 }
$$

- 绝对损失

$$
L ( Y , f ( X ) ) = \left| Y - f ( X ) \right|
$$

- 对数损失

$$
L ( Y , P ( Y \mid X ) ) = - \log P ( Y \mid X )
$$

#### ERM 与 SRM

经验风险最小化(ERM)与结构风险最小化(SRM)

1. **极大似然估计**是经验风险最小化的一个例子。

    当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化等价于极大似然估计

2. **贝叶斯估计**中的**最大后验概率估计**是结构风险最小化的一个例子。

    当模型是条件概率分布，损失函数是对数损失函数，**模型复杂度由模型的先验概率表示**时，结构风险最小化等价于最大后验概率估计。

### 算法


## 模型评估与模型选择

- 训练误差和测试误差是模型关于数据集的平均损失。

- 统计学习方法具体采用的损失函数未必是评估时使用的损失函数。

## 正则化与交叉验证

#### 正则化

模型选择的典型方法是正则化

#### 交叉验证

另一种常用的模型选择方法是交叉验证

- 简单
- S折(K折, K-Fold)
- 留一法

## 泛化能力

- 采用最多的方法是通过测试误差来评价学习方法的泛化能力

- 统计学习理论试图从理论上对学习方法的泛化能力进行分析

- 学习方法的泛化能力往往是通过研究泛化误差的**概率上界**进行的, 简称为泛化误差上界(generalization error bound)

**事实上，泛化误差就是所学习到的模型的期望风险**

## 生成模型与判别模型

**监督学习方法**可分为**生成方法**(generative approach)与**判别方法**(discriminative approach)。

||生成方法|判别方法|
|:---:|:---|:---|
|定义|由数据学习联合概率分布 $P(X, Y)$，然后求出条件概率分布 $P(Y \mid X)$ 作为预测的模型，即生成模型 $P(Y \mid X) = \frac { P(X, Y) } { P(X) }$|由数据直接学习决策函数 $f(X)$或者条件概率分布 $P(Y \mid X)$ 作为预测的模型|
|特点|1. 可以还原出联合概率分布 P(X, Y)；<br>2. 学习收敛速度更快；<br>3. 存在隐变量时，仍可以使用；|1. 直接面对预测，学习的准确率更高<br>2. 可以对数据进行各种程度的抽象, 定义特征并使用特征, 可以简化学习问题|


**10种统计学习方法总结**

|方法|适用问题|模型特点|模型类型|学习策略|损失函数|学习算法|
|:----|:----|:----|:----:|:----|:----|:----|
|感知机|二分类|分离超平面|判别模型|极小化误分点到超平面距离|误分点到超平面距离|随机梯度下降|
|k近邻法|多分类、回归|特征空间，样本点|判别模型|——|——|——|
|朴素贝叶斯法|多分类|特征与类别的联合概率分布，条件独立假设|生成模型|极大似然估计，极大后验概率估计|对数似然损失|概率计算公式，EM算法|
|决策树|多分类、回归|分类树，回归树|判别模型|正则化的极大似然估计|对数似然损失|特征选择，生成，剪枝|
|逻辑斯蒂回归与最大熵模型|多分类|特征条件下类别的条件概率分布，对数线性模型|判别模型|极大似然估计，正则化的极大似然估计|逻辑斯蒂损失|改进的迭代尺度算法，梯度下降，拟牛顿法|
|支持向量机|二分类|分离超平面，核技巧|判别模型|极小化正则化合页损失，软间隔最大化|合页损失|序列最小最优化算法（SMO）|
|提升方法|二分类|弱分类器的线性组合|判别模型|极小化加法模型的指数损失|指数损失|前向分布加法算法|
|EM算法|概率模型参数估计|含隐变量概率模型|——|极大似然估计，极大后验概率估计|对数似然损失|迭代算法|
|隐马尔可夫模型|标注|观测序列与状态序列的联合概率分布模型|生成模型|极大似然估计，极大后验概率估计|对数似然损失|概率计算公式，EM算法|
|条件随机场|标注|状态序列条件下观测序列的条件概率分布，对数线性模型|判别模型|极大似然估计，正则化极大似然估计|对数似然损失|改进的迭代尺度算法，梯度下降，拟牛顿法|